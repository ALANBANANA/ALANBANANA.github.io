# 第三章 线性回归

基本的形式：给定d个属性描述的示例$x = (x_1,;x_2;...x_d )$，其中$x_i$是x在第i个属性上的取值，线性模型试图学一个通过属性的线性组合来进行预测的函数
$$
f(x) = w_1x_1+w_2x_2+...+w_dx_d+b
$$
向量形式写成
$$
f(\bold{x}) = \bold{w}^T\bold{x}+b
$$


## 一元线性回归

例子：通过**发际线高度**预测**计算机水平**，每个人的发际线是散列分布在图上。众所周知，一个人的发际线越高，其计算机水平越强，成线性的关系，所以对于这种关系可以使用线性回归的模型去进行分析。

1. 仅通过**发际线高度**预测**计算机水平**：
   $$
   f(x) = w_1x_1+b
   $$

2. 加一些二值离散特征**颜值**（好看： 1， 不好看： 0）：
   $$
   f(x) = w_1x_1 + w_2x_2+ b
   $$

3. 再加有序的多值离散特征值**饭量**（小：1，中：2，大：3）：
   $$
   f(x) = w_1x_1+ w_2x_2+w_3x_3 +b
   $$

4. 再加无序的多值离散特征**肤色**（黄：[1, 0, 0]，黑：[0, 1, 0] ，白：[0, 0, 1]）
   $$
   f(x) = w_1x_1+ w_2x_2+w_3x_3 +w_4x_4+ w_5x5+w_6x_6+b
   $$

### 最小二乘法

对于这个散列分布的点其中越离线越近则说明效果越好，一般用**最小二乘估计**基于**均方误差最小化**
$$
\begin{align*}
E_{(w,b)} &= \sum_{i=1}^{m}{(y_i-f(x_i))^2} \\
&= \sum_{i=1}^{m}{(y_i-(wx_i+b))^2} \\
argmin_{(w,b)}&= \sum_{i=1}^{m}{(y_i-wx_i-b))^2}
\end{align*}
$$

### 极大似然估计

**用途**：估计概率论分布的参数值 -> **利用已知的样本结果，反推最大有可能（最大概率）导致这样结果的数值**

**方法**：对于离散型（连续型）随机变量$\bold{X}$，假设其概率质量函数为$P(x; \theta)$(概率密度函数为$p(x;\theta)$)，其中$\theta$为待估计的参数值(可以多个)。现有$x_1, x_2, ...，x_n$是来自$\bold{X}$的n个独立分布，它们的联合概率为：
$$
L(\theta) = \prod_{i=1}^{n}{P(x_i;\theta)}
$$
使观测样本出现概率最大的分布即为待求分布，也使得联合概率(似然函数)$L(\theta)$取到最大值$\theta^*$即为$\theta$的估计值

**实际最小化最小二乘法的值即为求解w和b**

梯度：(多元函数的一阶导数)

Hessian(海塞)矩阵：(多元函数的二阶导数)

Q1：为什么$argmin_{(w,b)}= \sum_{i=1}^{m}{(y_i-wx_i-b))^2}$是凸函数(最优化里面的概念，数学意义上的凹函数为最优化上的凸函数)？

Q2：知道凸函数之后，偏导数为0一定是最优解

所以以下为求解w和b的相关步骤：
$$
\begin{align*}
\frac{\partial E(w, b)}{\partial w} &= \frac{\partial}{\partial w}\left[\sum_{i=1}^{m}\left(y_i-wx_i-b\right)^2\right]\\
&=\sum_{i=1}^{m}\frac{\partial}{\partial w}(y_i-wx_i-b)^2\\
&=\sum_{i=1}^{m}2(y_i-wx_i-b)\cdot(-x_i)\\
&=2\left(w\sum_{i=1}^{m}x_i^2 - \sum_{i=1}^{m}(y_i-b)x_i\right)
\end{align*}
$$
再对上面式子求的偏导即对线性回归式子求二阶偏导：
$$
\begin{align*}
\frac{\partial ^2E(w, b)}{\partial w^2} &= \frac{\partial}{\partial w}(\frac{\partial E(w, b)}{\partial w})\\
&=\frac{\partial E(w, b)}{\partial w}\left[2\left(w\sum_{i=1}^{m}x_i^2 - \sum_{i=1}^{m}(y_i-b)x_i\right)\right]\\
&=\frac{\partial}{\partial w}(2w\sum_{i=1}^{m}x_i^2)\\
&=2\sum_{i=1}^{m}x_i^2
\end{align*}
$$
在一阶偏导的基础上对b进行偏导：
$$
\begin{align*}
\frac{\partial^2 E(w, b)}{\partial w \partial b} &= \frac{\partial}{\partial b}(\frac{\partial E(w, b)}{\partial w})\\
&=\frac{\partial E(w, b)}{\partial b}\left[2\left(w\sum_{i=1}^{m}x_i^2 - \sum_{i=1}^{m}(y_i-b)x_i\right)\right]\\
&= \frac{\partial}{\partial b}\left[-2\sum_{i=1}^{m}(y_i-b)x_i\right] \\
&=\left(-2\sum_{i=1}^{m}y_ix_i+2\sum_{i=1}^{m}bx_i \right) \\ 
&=2\sum_{i=1}^{m}x_i
\end{align*}
$$
再对b进行单独求导：
$$
\begin{align*}
\frac{\partial E(w, b)}{\partial b} &= \frac{\partial}{\partial b}\left[\sum_{i=1}^{m}\left(y_i-wx_i-b\right)^2\right] \\
&=\sum_{i=1}^{m}\frac{\partial}{\partial b}(y_i-wx_i-b)(-1)\\
&= 2\left(mb-\sum_{i=1}^{m}(y_i-wx_i)\right)
\end{align*}
$$
以及以下：
$$
\begin{align*}
\frac{\partial ^2E(w, b)}{\partial b \partial w} &= \frac{\partial}{\partial w}(\frac{\partial E(w, b)}{\partial b}) \\
&= \frac{\partial}{\partial w}\left[2\left(mb-\sum_{i=1}^{m}(y_i-wx_i)\right)\right]\\
&=\frac{\partial}{\partial w}(2\sum_{i=1}^{m}wx_i)\\
&=2\sum_{i=1}^{m}x_i
\end{align*}
$$
最后对b求其二阶导：
$$
\begin{align*}
\frac{\partial ^2E(w, b)}{\partial b^2} &= \frac{\partial}{\partial b}(\frac{\partial E(w, b)}{\partial b}) \\
&= \frac{\partial}{\partial b}\left[2\left(mb-\sum_{i=1}^{m}(y_i-wx_i)\right)\right]\\
&=2m
\end{align*}
$$
所以接下来即判定以下半正定举证是否为大于0的矩阵：若实对称矩阵的所有顺序主子式均为非负，则该矩阵为半正定矩阵
$$
\nabla^2E_{(w, b)} &= \left[
\begin{array}{cc}
\frac{\partial ^2E(w, b)}{\partial w^2} &\frac{\partial^2 E(w, b)}{\partial w \partial b}\\\\
\frac{\partial ^2E(w, b)}{\partial b \partial w} & \frac{\partial ^2E(w, b)}{\partial b^2}
\end{array}
\right] = 
\left[\begin{array}{cc}
2\sum_{i=1}^{m}x_i^2 & 2\sum_{i=1}^{m}x_i\\\\
2\sum_{i=1}^{m}x_i & 2m
\end{array}\right] \\&= 4m\sum_{i=1}^{m}x_i^2-4\left(\sum_{i=1}^{m}x_i\right)^2\geq0
$$
凸充分定理：若$f: \mathbb{R}^n \to \mathbb{R}$是凸函数，切$f(X)$是一阶连续可微， 则$x^*$是全局解的充分必要条件**$\nabla f(x^*) = \bold{0}$**, 所以$\nabla E_{(w, b)} = 0$ 的点即为最小值点， 其关于w, b的一阶导数全为**0**

求解w, b:

先对b进行求解：
$$
\begin{gather*}
\frac{\partial E(w, b)}{\partial b} = 2\left(mb-\sum_{i=1}^{m}(y_i-wx_i)\right)\\
mb -\sum_{i=1}^{m}(y_i-wx_i) = 0 \\
b = \frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i)\\
b = \frac{1}{m}\sum_{i=1}^{m}y_i-w\sum_{i=1}^{m}x_i = \overline{y} - w\overline{x}
\end{gather*}
$$


所以$b=\overline{y} - w\overline{x}$
$$
\begin{gather*}
\frac{\partial E(w, b)}{\partial w} =2\left(w\sum_{i=1}^{m}x_i^2 - \sum_{i=1}^{m}(y_i-b)x_i\right)=0\\
w\sum_{i=1}^{m}x_i^2 - \sum_{i=1}^{m}(y_i-b)x_i = 0\\
w\sum_{i=1}^{m}x_i^2 = \sum_{i=1}^{m}y_ix_i-\sum_{i=1}^{m}bx_i
\end{gather*}
$$
将$b = \overline{y} - w\overline{x}$代入上式可得：
$$
\begin{gather*}
w\sum_{i=1}^{m}x_i^2 = \sum_{i=1}^{m}y_ix_i-\sum_{i=1}^{m}(\overline{y} - w\overline{x})x_i \\
w\sum_{i=1}^{m}x_i^2 = \sum_{i=1}^{m}y_ix_i-\overline{y}\sum_{i=1}^{m} x_i+ w\overline{x}\sum_{i=1}^{m}x_i\\
w\sum_{i=1}^{m}x_i^2 -w\overline{x}\sum_{i=1}^{m}x_i =  \sum_{i=1}^{m}y_ix_i-\overline{y}\sum_{i=1}^{m}x_i\\
w\left(\sum_{i=1}^{m}x_i^2 -\overline{x}\sum_{i=1}^{m}x_i\right) =  \sum_{i=1}^{m}y_ix_i-\overline{y}\sum_{i=1}^{m}x_i\\
w = \frac{\sum_{i=1}^{m}y_ix_i-\overline{y}\sum_{i=1}^{m}x_i}{\sum_{i=1}^{m}x_i^2 -\overline{x}\sum_{i=1}^{m}x_i} = \frac{\sum_{i=1}^{m}y_ix_i-\overline{x}\sum_{i=1}^{m}y_i}{\sum_{i=1}^{m}x_i^2 -\frac{1}{m}\left(\sum_{i=1}^{m}x_i\right)^2}
\end{gather*}
$$


假如$\sum$太多则可能单纯使用自带库，则循环太多时间复杂度则会增加，可以直接使用numpy进行矩阵运算

### 机器学习三要素

1. 模型：根据具体问题，确定假设空间
2. 策略：根据评价标准，确定选取最优模型的策略(通常会产出一个“损失函数”)
3. 算法：求解损失函数，确定最优模型

## 多元线性回归

容易理解多元线性回归本质就是多个维度(多个未知量x)的线性回归：
$$
\begin{gather*}
f(\bold{x_i}) = w_1x_{i1}+w_2x_{i2}+...+w_dx_{id}+b
f(\bold{x_i}) = w_1x_{i1}+w_2x_{i2}+...+w_dx_{id}+w_{d+1}\cdot1
\end{gather*}
$$
由线性代数上述的式子可以写为向量内积的形式：
$$
f(\bold{x_i}) = \left(w_1\quad w_2 ...w_d\quad w_{d+1}\right)\left(
\begin{array}{c}
x_{i1}\\\\
x_{i2}\\\\
\vdots\\\\
x_{id}\\\\
1
\end{array}
\right)\\
$$
上述式子写成大写字母的向量形式即为：$f(\hat{\bold{x_i}}) = \hat{\bold{w}}^T\hat{\bold{x}}_i$

由最小二乘法：
$$
\begin{gather*}
E_\hat{w} = \sum_{i=1}^{m}\left(y_i-f(\hat{\bold{x_i}})\right)^2 = \sum_{i=1}^{m}\left(y_i-\hat{\bold{w}}^T\hat{\bold{x}}_i\right)^2\\
\sum_{i=1}^{m}\left(y_i-\hat{\bold{w}}^T\hat{\bold{x}}_i\right)^2 = \left(y_1-\hat{\bold{w}}^T\hat{\bold{x}}_1\right)^2 + \left(y_2-\hat{\bold{w}}^T\hat{\bold{x}}_2\right)^2+...+\left(y_m-\hat{\bold{w}}^T\hat{\bold{x}}_m\right)^2\\
E_\hat{w} = (y_1-\hat{\bold{w}}^T\hat{\bold{x}}_1 \quad y_2-\hat{\bold{w}}^T\hat{\bold{x}}_2 \quad ... \quad y_m-\hat{\bold{w}}^T\hat{\bold{x}}_m)
\left(
\begin{array}{c}
y_1-\hat{\bold{w}}^T\hat{\bold{x}}_1 \\\\
y_2-\hat{\bold{w}}^T\hat{\bold{x}}_2 \\\\
\vdots \\\\
\quad y_m-\hat{\bold{w}}^T\hat{\bold{x}}_m
\end{array}
\right)\\
E_\hat{w} = (\bold{y}-\bold{X}\bold{\hat{w}})^T(\bold{y}-\bold{X}\bold{\hat{w}})
\end{gather*}
$$
所以就是求解$\hat{w}^* = argmin_{w^*}(\bold{y}-\bold{X}\bold{\hat{w}})^T(\bold{y}-\bold{X}\bold{\hat{w}})$

则按照一元线性回归，一阶求解为0可知：
$$
\frac{\partial E_\hat{w}}{\partial \bold{\hat{w}}} = \frac{\partial}{\partial \bold{\hat{w}}}[(\bold{y}-\bold{X}\bold{\hat{w}})^T(\bold{y}-\bold{X}\bold{\hat{w}})] = 2\bold{X}^T(\bold{X}\hat{\bold{w}}-\bold{y})
$$


海塞矩阵：
$$
\begin{align*}
\nabla^2E_\hat{w} &= \frac{\partial}{\partial \bold{\hat{w}}}\left(\frac{\partial E_\hat{w}}{\partial \bold{\hat{w}}}\right)\\
&= \frac{\partial}{\partial \bold{\hat{w}}}[2\bold{X}^T(\bold{X}\hat{\bold{w}}-\bold{y})]\\
&=\frac{\partial}{\partial \bold{\hat{w}}}(2\bold{X}^T\bold{X}\hat{\bold{w}}-2\bold{X}^T\bold{y})
\end{align*}
$$
假定$\bold{X}^T\bold{X}$为正定矩阵， 则$E_\hat{w}$则为凸函数得证， 所以令一阶导为0：
$$
\begin{gather*}
\frac{\partial E_\hat{w}}{\partial \bold{\hat{w}}} = 2\bold{X}^T(\bold{X}\hat{\bold{w}}-\bold{y})\\
2\bold{X}^T(\bold{X}\hat{\bold{w}}-\bold{y}) = 0\\
2\bold{X}^T\bold{X}\hat{\bold{w}}=2\bold{X}^T\bold{y}\\
\hat{\bold{w}} = (\bold{X}^T\bold{X})^{-1}\bold{X}^T\bold{y}
\end{gather*}
$$

## 对数回归

### 认识对数回归的样式，什么是Sigmoid函数

考虑二分类问题，上述的回归学习并不太适用于分类问题的任务。

考虑二分类任务，输出指标为0和1，而线性回归模型产生的预测值$z=\bold{w}^T\bold{x}+b$，将实值转变为0/1值，最理想的"阶跃函数"如下：
$$
\begin{equation*}
y=
\begin{cases}
0, & \text{z<0}\\
0.5, & \text{z=0}\\
1, & \text{z>1}
\end{cases}
\end{equation*}
$$
![classification1](images/classification1.png)

然而单位阶跃函数并不是连续的，而对数几率函数可以对此成果进行微调
$$
y = \frac{1}{1+e^{-z}}
$$
该对数几率函数实际上是**Sigmoid函数**，可以将z值转化为接近为0或者1的y值，并且其输出值在z=0附近变化很逗，因此有如下式子
$$
\begin{gather*}
y  = \frac{1}{1+e^{-(\bold{w}^T\bold{x}+b)}}\\
转化为 \to ln(\frac{y}{1-y}) = \bold{w}^T\bold{x}+b \\
\end{gather*}
$$
对数几率函数是任意阶的可导凸函数。上述式子写成类后验概率估计$p(y=1|x)$，则可以重写为：
$$
\begin{align*}
\ln\frac{p(y=1 | x)}{p(y=0|x)} &= \bold{w}^T\bold{x}+b\\
p(y=1 | x) &= \frac{e^{(\bold{w}^T\bold{x}+b)}}{1+e^{(\bold{w}^T\bold{x}+b)}} \\
p(y=1 | x) &= \frac{1}{1+e^{(\bold{w}^T\bold{x}+b)}}\\
\ln\frac{p(y=1 | x)}{1-p(y=1|x)} &=\bold{w}^T\bold{x}+b\\
\end{align*}
$$


### 最优化对数分类函数的方法

可以通过极大似然估计进行w和b的方法来进行最大化"对数似然"：
$$
l(w, b) = \sum_{i=1}^{m}\ln p(y_i|x_i;w,b)
$$
其中将$\beta = (w;b)$，$\hat{x} = (x;1)$，则$\bold{w}^T\bold{x}+b$简写为$\bold{\beta}^T\hat{x}$，$p_1(\hat{x};\beta) = p(y=1|\hat{x};\beta)$，$p_0(\hat{x};\beta)=p(y=0|\hat{x};\beta) = 1-p_1$，则上述式子重写为：
$$
l(\beta) = \sum_{i=1}^{m}(-y_i\beta^T\hat{x_i}+\ln(1+e^{\beta^T\hat{x_i}}))
$$

### 关于求解高阶可导函数（牛顿法）

由凸函数理论，经典的数值优化如梯度下降，牛顿法等等可以优化求得最优解，对上述的式子的优化即为：
$$
\beta^* = argmin_\beta\quad l(\beta)
$$
以牛顿法为例：
$$
\beta^{t+1} = \beta^t - \left( \frac{\partial^2l(\beta)}{\partial \beta\partial\beta^T}\right) \frac{\partial l(\beta)}{\partial \beta}
$$
关于$\beta$的一阶、二阶导数为：
$$
\begin{align*}
\frac{\partial l(\beta)}{\partial \beta} &= -\sum_{i=1}^{m}\hat{x_i}(y_i-p_1(\hat{x_i};\beta))\\
\frac{\partial^2 l(\beta)}{\partial \beta\partial \beta^T} &= \sum_{i=1}^{m}\hat{x_i}\hat{x_i}^Tp_1(\hat{x_i};\beta)(y_i-p_1(\hat{x_i};\beta))
\end{align*}
$$
