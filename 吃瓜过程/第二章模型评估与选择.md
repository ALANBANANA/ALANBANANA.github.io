# 第二章 模型评估与选择

## 经验误差和过拟合

1. 错误率： 分类错误的样本数站样本总数的比例称为**“错误率”**，即m个样本中有a个样本分类错误，则错误率即为$E = a/m$，对此**精度**即为$1-\frac{a}{m}$
2. 误差为学习器的实际预测输出与样本的真实输出之间的差异
3. **训练误差**是学习器在训练集上的经验误差、
4. **泛化误差**是在新样本上的误差

对于错误率和精度，实际在分类的问题上很好的理解，而对于误差的概念反倒更加适用于回归问题

**过拟合与欠拟合**

1. 学习器学习训练样本学得“太好”， 那么就会对测试样本的泛化能力下降，这是**过拟合**
2. 学习器学习训练样本学得“不太好”，对于数据来说有点低下，这是**欠拟合**

![whetherFirOrNot](images/whetherfit.png)

在上面这个图中，假如说点为数据集，那么A线对于数据来说是比较符合拟合的，而B点就显得过拟合了，每一个点都经过了

## 评估方法

通过实验测试来对学习器的泛化误差进行评估并进行评估而做出选择，因此，需要适用一个"测试集"来测试学习器对新样本的判别能力，再以测试集上的"测试误差"来作为泛化误差的近似。**测试集要尽可能与训练集互斥**（尽量测试集的数据不要出现再训练集中），所以会采取以下三种方式

### 留出法

直接将数据集$D$划分为两个互斥的集合，其中一个集合为训练集$S$，另一个集合为测试集$T$， 这个比较好理解，但是训练和测试的划分要尽量保持数据的一致性，避免因为数据划分过程引入额外偏差而最终产生影响，如数据集中500个反例，500个正例，那么就是按照7：3去划分即为350个正/反例子作训练集，和150个正/反例子作测试集，同理500个正例中前350作训练或者后359做训练，结果也会有些许差异

### 交叉验证法

直接将数据集$D$划分为k个大小相似得互斥子集，$D=D_1\cup D_2\cup D_3...\cup D_k$，$ D_i \cap D_j\neq \varnothing(i \neq j)$，每个子集都可以保持数据分布得一致性，即从D中通过分层采样，然后用k-1个子集作为测试集，如此即可获得k组训练/测试集，进行k次训练和测试，最终返回k个测试结果得均值

![crossCheck](images/crosscheck.png)



### 自助法(Bootstrapping)：有放回得采样方式

给定包含m个样本得数据集D， 我们对它进行采样产生的数据集D'：每次随机从D中挑选一个样本，将其拷贝到D'，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍然可能被采到；这个过程重复执行m次后，我们就得到包含m个样本得数据集D'，这就是自主采样
$$
\lim\limits_{m \to \infty}\left(1-\frac{1}{m}\right)^m\to\frac{1}{e}\approx 0.368
$$
通过对数据进行bootstrapping所得的数据可以得出将近有$1/3$的数据将会被用到测试集当中

## 性能度量

类似回归的学习器的机器学习任务一般用**均方误差**来描述一个机器学习任务是否符合当前学习任务是否可行的衡量标准
$$
E(f;D) = \frac{1}{m}\sum_{i=1}^{n}\left(f(x_i)-y_i\right)
$$
但对于分类问题上的误差衡量实际上用**查准率**进行度量学习器的优良更为合适，正例(TP)、假正例(FP)、真反例(TN)、假反例(FN)

查准率P和查全率R由下面公式
$$
P = \frac{TP}{TP+FP},\quad
R = \frac{TP}{TP+FN}
$$
可以根据学习器的预测结果对样例进行排序，排在前面的学习器可以被认为是”最可能“是正例的样本，繁殖则是”最不可能“的正例样本，下面则为关于P-R的示意图

![blancePoint](images/blancepoint.png)

**ROC与AUC**

ROC为受试者工作特征曲线， ROC曲线图中横轴为"假正例率"(FPR)，纵轴为"真正例率"(TPR)
$$
TFR = \frac{TP}{TP+FN}, \quad
FPR = \frac{FP}{TN+FP}
$$
进行学习器比较时，与P-R图相似，若一个学习器的ROC曲线被另一个学习器的曲线包围住。则可以断言后者性能要优于前者性能，若两个学习器的ROC发生曲线交叉，难以分辨则用AUC，AUC可以通过对ROC的曲线求和而得，AUC考虑得是样本预测的排序质量，与排序误差紧密相关
